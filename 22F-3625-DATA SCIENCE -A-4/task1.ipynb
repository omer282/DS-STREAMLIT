{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3LIxX2CcMfWg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import joblib\n",
        "\n",
        "# Step 1: Set the path to the local dataset folder\n",
        "desktop_path = os.path.expanduser(\"~/Desktop/22F-3625-DS-A-04/deepfake_detection_dataset_urdu\")  # Portable path to Desktop folder\n",
        "\n",
        "# Step 2: Define subfolder paths\n",
        "bonafide_dir = os.path.join(desktop_path, \"Bonafide\")\n",
        "spoofed_tacotron_dir = os.path.join(desktop_path, \"Spoofed_Tacotron\")\n",
        "spoofed_tts_dir = os.path.join(desktop_path, \"Spoofed_TTS\")\n",
        "\n",
        "# Step 3: Feature Extraction Function (MFCCs)\n",
        "def extract_mfcc(file_path, max_len=300):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "        if mfcc.shape[1] < max_len:\n",
        "            pad_width = max_len - mfcc.shape[1]\n",
        "            mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mfcc = mfcc[:, :max_len]\n",
        "        return mfcc.flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Extract Features & Labels\n",
        "X, y = [], []\n",
        "label_map = {\"bonafide\": 0, \"spoof\": 1}\n",
        "\n",
        "# Helper function to process all .wav files in a directory (for Part 1 and Part 2)\n",
        "def process_wav_files(directory, label, desc):\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "        return\n",
        "\n",
        "    # Get list of .wav files in the directory\n",
        "    wav_files = [f for f in os.listdir(directory) if f.endswith(\".wav\")]\n",
        "\n",
        "    # Process each .wav file with progress bar\n",
        "    for wav_file in tqdm(wav_files, desc=desc):\n",
        "        file_path = os.path.join(directory, wav_file)\n",
        "        features = extract_mfcc(file_path)\n",
        "        if features is not None:\n",
        "            X.append(features)\n",
        "            y.append(label)\n",
        "\n",
        "# Helper function to process all .wav files in a Speaker_XX folder (for Bonafide)\n",
        "def process_bonafide_speaker_folder(speaker_dir, desc):\n",
        "    if not os.path.exists(speaker_dir):\n",
        "        print(f\"Directory not found: {speaker_dir}\")\n",
        "        return\n",
        "\n",
        "    # Get list of Speaker_XX folders\n",
        "    speaker_folders = [f for f in os.listdir(speaker_dir) if f.startswith(\"Speaker_\") and os.path.isdir(os.path.join(speaker_dir, f))]\n",
        "\n",
        "    # Process each Speaker_XX folder\n",
        "    for speaker_folder in tqdm(speaker_folders, desc=desc):\n",
        "        speaker_path = os.path.join(speaker_dir, speaker_folder)\n",
        "        # Process Part 1 and Part 2 within each Speaker_XX folder\n",
        "        part1_dir = os.path.join(speaker_path, \"Part 1\")\n",
        "        part2_dir = os.path.join(speaker_path, \"Part 2\")\n",
        "        process_wav_files(part1_dir, label_map[\"bonafide\"], f\"Processing {speaker_folder} Part 1\")\n",
        "        process_wav_files(part2_dir, label_map[\"bonafide\"], f\"Processing {speaker_folder} Part 2\")\n",
        "\n",
        "# Helper function to process all .wav files in a Speaker_XX folder (for Spoofed_*)\n",
        "def process_spoofed_speaker_folder(speaker_dir, label, desc):\n",
        "    if not os.path.exists(speaker_dir):\n",
        "        print(f\"Directory not found: {speaker_dir}\")\n",
        "        return\n",
        "\n",
        "    # Get list of Speaker_XX folders\n",
        "    speaker_folders = [f for f in os.listdir(speaker_dir) if f.startswith(\"Speaker_\") and os.path.isdir(os.path.join(speaker_dir, f))]\n",
        "\n",
        "    # Process each Speaker_XX folder\n",
        "    for speaker_folder in tqdm(speaker_folders, desc=desc):\n",
        "        speaker_path = os.path.join(speaker_dir, speaker_folder)\n",
        "        # Get list of .wav files in the Speaker_XX folder\n",
        "        wav_files = [f for f in os.listdir(speaker_path) if f.endswith(\".wav\")]\n",
        "        for wav_file in wav_files:\n",
        "            file_path = os.path.join(speaker_path, wav_file)\n",
        "            features = extract_mfcc(file_path)\n",
        "            if features is not None:\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "# Process Bonafide files (Speaker_XX/Part 1 and Part 2)\n",
        "process_bonafide_speaker_folder(bonafide_dir, \"Processing Bonafide\")\n",
        "\n",
        "# Process Spoofed_Tacotron files\n",
        "process_spoofed_speaker_folder(spoofed_tacotron_dir, label_map[\"spoof\"], \"Processing Spoofed_Tacotron\")\n",
        "\n",
        "# Process Spoofed_TTS files\n",
        "process_spoofed_speaker_folder(spoofed_tts_dir, label_map[\"spoof\"], \"Processing Spoofed_TTS\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Step 5: Check if data is loaded\n",
        "if len(X) == 0 or len(y) == 0:\n",
        "    raise ValueError(\"No valid audio files were processed. Check file paths and formats.\")\n",
        "\n",
        "# Step 6: Scale Features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Step 7: Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 8: Evaluation Function\n",
        "def evaluate_model(name, model, X_test, y_test, probas=None):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    if probas is not None:\n",
        "        print(\"AUC-ROC:\", roc_auc_score(y_test, probas))\n",
        "\n",
        "# Step 9: Train SVM\n",
        "svm_model = SVC(probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "evaluate_model(\"SVM\", svm_model, X_test, y_test, svm_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# Step 10: Train Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "evaluate_model(\"Logistic Regression\", lr_model, X_test, y_test, lr_model.predict_proba(X_test)[:, 1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "voGqGXipNP5F",
        "outputId": "089e50cb-1c93-46cb-94c2-229d5dbf70e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory not found: /root/Desktop/22F-3625-DS-A-04/deepfake_detection_dataset_urdu/Bonafide\n",
            "Directory not found: /root/Desktop/22F-3625-DS-A-04/deepfake_detection_dataset_urdu/Spoofed_Tacotron\n",
            "Directory not found: /root/Desktop/22F-3625-DS-A-04/deepfake_detection_dataset_urdu/Spoofed_TTS\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No valid audio files were processed. Check file paths and formats.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a3b41467ccd3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Step 5: Check if data is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No valid audio files were processed. Check file paths and formats.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Step 6: Scale Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No valid audio files were processed. Check file paths and formats."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Train Perceptron\n",
        "perceptron_model = Perceptron(max_iter=1000)\n",
        "perceptron_model.fit(X_train, y_train)\n",
        "evaluate_model(\"Perceptron\", perceptron_model, X_test, y_test)\n",
        "\n",
        "# Step 12: Define DNN Class\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Step 13: Prepare Data for PyTorch\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "# Step 14: Train DNN\n",
        "dnn_model = DNN(X.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(dnn_model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"\\nTraining DNN:\")\n",
        "for epoch in range(10):\n",
        "    dnn_model.train()\n",
        "    outputs = dnn_model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Step 15: Evaluate DNN\n",
        "dnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_probs = dnn_model(X_test_tensor).numpy().flatten()\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nDNN Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_probs))\n",
        "\n",
        "# Step 16: Save Models\n",
        "joblib.dump(lr_model, \"lr_audio_model.joblib\")\n",
        "joblib.dump(svm_model, \"svm_audio_model.joblib\")\n",
        "joblib.dump(perceptron_model, \"perceptron_audio_model.joblib\")\n",
        "torch.save(dnn_model.state_dict(), \"dnn_audio_model.pth\")\n",
        "joblib.dump(scaler, \"audio_scaler.joblib\")\n",
        "print(\"\\nModels saved successfully for Part 1!\")"
      ],
      "metadata": {
        "id": "2w0aPeVdNca9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}