{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Ty7YL1OOGJy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import hamming_loss, f1_score, precision_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import joblib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Load and Preprocess the Dataset\n",
        "def load_and_preprocess_data():\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "    # Check for missing values in labels\n",
        "    label_cols = [col for col in df.columns if col.startswith(\"type_\")]\n",
        "    print(\"Missing values in labels:\", df[label_cols].isnull().sum())\n",
        "    df = df.dropna(subset=label_cols)  # Drop rows with missing labels\n",
        "\n",
        "    # Extract features using TF-IDF with reduced features\n",
        "    vectorizer = TfidfVectorizer(max_features=500, stop_words=\"english\")\n",
        "    X = vectorizer.fit_transform(df[\"report\"]).toarray()\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Extract labels\n",
        "    y = df[label_cols].values\n",
        "\n",
        "    # Analyze label distribution\n",
        "    print(\"\\nLabel Distribution:\")\n",
        "    for i, col in enumerate(label_cols):\n",
        "        print(f\"{col}: {np.sum(y[:, i])} positive samples ({np.sum(y[:, i]) / len(y) * 100:.2f}%)\")\n",
        "\n",
        "    return X, y, label_cols, vectorizer, scaler\n",
        "\n",
        "# Step 2: Split the Data\n",
        "def split_data(X, y):\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42)  # 0.1765 of 85% = 15%\n",
        "    print(f\"\\nTrain: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Step 3: Define Evaluation Metrics\n",
        "def evaluate_model(y_true, y_pred, label_cols, k=3):\n",
        "    # Hamming Loss\n",
        "    hamming = hamming_loss(y_true, y_pred)\n",
        "\n",
        "    # Micro and Macro F1\n",
        "    micro_f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
        "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "    # Precision@k\n",
        "    precision_k = 0\n",
        "    for i in range(len(y_true)):\n",
        "        true_labels = np.where(y_true[i] == 1)[0]\n",
        "        pred_labels = np.argsort(y_pred[i])[::-1][:k]  # Top k predictions\n",
        "        precision_k += len(set(true_labels).intersection(pred_labels)) / k\n",
        "    precision_k /= len(y_true)\n",
        "\n",
        "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
        "    print(f\"Micro-F1: {micro_f1:.4f}\")\n",
        "    print(f\"Macro-F1: {macro_f1:.4f}\")\n",
        "    print(f\"Precision@{k}: {precision_k:.4f}\")\n",
        "    return hamming, micro_f1, macro_f1, precision_k\n",
        "\n",
        "# Step 4: Logistic Regression\n",
        "def train_logistic_regression(X_train, y_train, X_val, y_val, label_cols):\n",
        "    print(\"\\nTraining Logistic Regression...\")\n",
        "    param_grid = {\"estimator__C\": [0.1, 1, 10]}\n",
        "    base_lr = LogisticRegression(max_iter=2000)\n",
        "    model = OneVsRestClassifier(base_lr)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"f1_micro\", n_jobs=-1, verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best C: {grid_search.best_params_['estimator__C']}\")\n",
        "    y_pred = grid_search.predict(X_val)\n",
        "    print(\"Logistic Regression Validation Results:\")\n",
        "    evaluate_model(y_val, y_pred, label_cols)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Step 5: SVM\n",
        "def train_svm(X_train, y_train, X_val, y_val, label_cols):\n",
        "    print(\"\\nTraining SVM...\")\n",
        "    param_grid = {\"estimator__C\": [1]}  # Simplified to single value for speed\n",
        "    base_svm = LinearSVC(max_iter=1000, random_state=42, dual=\"auto\")\n",
        "    model = OneVsRestClassifier(base_svm)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring=\"f1_micro\", n_jobs=-1, verbose=1)\n",
        "\n",
        "    print(\"Starting GridSearchCV...\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"GridSearchCV completed.\")\n",
        "\n",
        "    print(f\"Best C: {grid_search.best_params_['estimator__C']}\")\n",
        "    y_pred = grid_search.predict(X_val)\n",
        "    print(\"SVM Validation Results:\")\n",
        "    evaluate_model(y_val, y_pred, label_cols)\n",
        "    return grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Custom Perceptron for Online Learning\n",
        "class MultiLabelPerceptron:\n",
        "    def __init__(self, n_features, n_labels, learning_rate=0.01):\n",
        "        self.weights = np.zeros((n_labels, n_features))\n",
        "        self.bias = np.zeros(n_labels)\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        scores = X @ self.weights.T + self.bias\n",
        "        return (scores > 0).astype(int)\n",
        "\n",
        "    def update(self, x, y_true):\n",
        "        y_pred = self.predict(x.reshape(1, -1))[0]\n",
        "        for label in range(len(y_true)):\n",
        "            if y_true[label] != y_pred[label]:\n",
        "                self.weights[label] += self.lr * (y_true[label] - y_pred[label]) * x\n",
        "                self.bias[label] += self.lr * (y_true[label] - y_pred[label])\n",
        "\n",
        "def train_perceptron(X_train, y_train, X_val, y_val, label_cols):\n",
        "    print(\"\\nTraining Perceptron (Online Learning)...\")\n",
        "    learning_rates = [0.001, 0.01, 0.1]\n",
        "    best_lr, best_f1 = None, 0\n",
        "    best_model = None\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        model = MultiLabelPerceptron(X_train.shape[1], y_train.shape[1], learning_rate=lr)\n",
        "        # Online learning: Update after each sample\n",
        "        for epoch in range(10):\n",
        "            for i in range(len(X_train)):\n",
        "                model.update(X_train[i], y_train[i])\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        micro_f1 = f1_score(y_val, y_pred, average=\"micro\")\n",
        "        if micro_f1 > best_f1:\n",
        "            best_f1 = micro_f1\n",
        "            best_lr = lr\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"Best Learning Rate: {best_lr}\")\n",
        "    y_pred = best_model.predict(X_val)\n",
        "    print(\"Perceptron Validation Results:\")\n",
        "    evaluate_model(y_val, y_pred, label_cols)\n",
        "    return best_model\n",
        "\n",
        "# Step 7: Deep Neural Network\n",
        "class MultiLabelDNN(nn.Module):\n",
        "    def __init__(self, input_dim, n_labels):\n",
        "        super(MultiLabelDNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, n_labels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_dnn(X_train, y_train, X_val, y_val, label_cols):\n",
        "    print(\"\\nTraining DNN...\")\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultiLabelDNN(X_train.shape[1], y_train.shape[1])\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    batch_size = 32\n",
        "    num_samples = X_train_tensor.shape[0]\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    for epoch in range(50):\n",
        "        model.train()\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min(start_idx + batch_size, num_samples)\n",
        "            batch_X = X_train_tensor[start_idx:end_idx]\n",
        "            batch_y = y_train_tensor[start_idx:end_idx]\n",
        "\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_tensor)\n",
        "            val_loss = criterion(val_outputs, y_val_tensor)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Validation Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_probs = model(X_val_tensor).numpy()\n",
        "        y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    print(\"DNN Validation Results:\")\n",
        "    evaluate_model(y_val, y_pred, label_cols)\n",
        "    return model\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    X, y, label_cols, vectorizer, scaler = load_and_preprocess_data()\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "\n",
        "    # Train models\n",
        "    lr_model = train_logistic_regression(X_train, y_train, X_val, y_val, label_cols)\n",
        "    svm_model = train_svm(X_train, y_train, X_val, y_val, label_cols)\n",
        "    perceptron_model = train_perceptron(X_train, y_train, X_val, y_val, label_cols)\n",
        "    dnn_model = train_dnn(X_train, y_train, X_val, y_val, label_cols)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"\\n=== Test Set Evaluation ===\")\n",
        "    for name, model in [(\"Logistic Regression\", lr_model), (\"SVM\", svm_model)]:\n",
        "        print(f\"\\n{name}:\")\n",
        "        y_pred = model.predict(X_test)\n",
        "        evaluate_model(y_test, y_pred, label_cols)\n",
        "\n",
        "    print(\"\\nPerceptron:\")\n",
        "    y_pred = perceptron_model.predict(X_test)\n",
        "    evaluate_model(y_test, y_pred, label_cols)\n",
        "\n",
        "    print(\"\\nDNN:\")\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    dnn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_probs = dnn_model(X_test_tensor).numpy()\n",
        "        y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "    evaluate_model(y_test, y_pred, label_cols)\n",
        "\n",
        "    # Save Models\n",
        "    joblib.dump(lr_model, \"lr_defect_model.joblib\")\n",
        "    joblib.dump(svm_model, \"svm_defect_model.joblib\")\n",
        "    joblib.dump(perceptron_model, \"perceptron_defect_model.joblib\")\n",
        "    torch.save(dnn_model.state_dict(), \"dnn_defect_model.pth\")\n",
        "    joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
        "    joblib.dump(scaler, \"scaler.joblib\")\n",
        "    print(\"\\nModels saved successfully for Part 2!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E6TaujoO7Mi",
        "outputId": "4e23ce5d-7fd4-4e38-a037-26890ac1a180"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in labels: type_blocker               0\n",
            "type_regression            0\n",
            "type_bug                   0\n",
            "type_documentation         0\n",
            "type_enhancement           0\n",
            "type_task                  0\n",
            "type_dependency_upgrade    0\n",
            "dtype: int64\n",
            "\n",
            "Label Distribution:\n",
            "type_blocker: 134 positive samples (9.67%)\n",
            "type_regression: 115 positive samples (8.30%)\n",
            "type_bug: 676 positive samples (48.77%)\n",
            "type_documentation: 1134 positive samples (81.82%)\n",
            "type_enhancement: 676 positive samples (48.77%)\n",
            "type_task: 0 positive samples (0.00%)\n",
            "type_dependency_upgrade: 43 positive samples (3.10%)\n",
            "\n",
            "Train: 970, Validation: 208, Test: 208\n",
            "\n",
            "Training Logistic Regression...\n",
            "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
            "Best C: 0.1\n",
            "Logistic Regression Validation Results:\n",
            "Hamming Loss: 0.1339\n",
            "Micro-F1: 0.7619\n",
            "Macro-F1: 0.5407\n",
            "Precision@3: 0.5080\n",
            "\n",
            "Training SVM...\n",
            "Starting GridSearchCV...\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "GridSearchCV completed.\n",
            "Best C: 1\n",
            "SVM Validation Results:\n",
            "Hamming Loss: 0.1559\n",
            "Micro-F1: 0.7326\n",
            "Macro-F1: 0.5200\n",
            "Precision@3: 0.5048\n",
            "\n",
            "Training Perceptron (Online Learning)...\n",
            "Best Learning Rate: 0.001\n",
            "Perceptron Validation Results:\n",
            "Hamming Loss: 0.1635\n",
            "Micro-F1: 0.7271\n",
            "Macro-F1: 0.5029\n",
            "Precision@3: 0.5128\n",
            "\n",
            "Training DNN...\n",
            "Epoch 10, Validation Loss: 0.3684\n",
            "Epoch 20, Validation Loss: 0.5409\n",
            "Epoch 30, Validation Loss: 0.6151\n",
            "Epoch 40, Validation Loss: 0.7058\n",
            "Epoch 50, Validation Loss: 0.7710\n",
            "DNN Validation Results:\n",
            "Hamming Loss: 0.1147\n",
            "Micro-F1: 0.7971\n",
            "Macro-F1: 0.6112\n",
            "Precision@3: 0.5304\n",
            "\n",
            "=== Test Set Evaluation ===\n",
            "\n",
            "Logistic Regression:\n",
            "Hamming Loss: 0.1250\n",
            "Micro-F1: 0.7797\n",
            "Macro-F1: 0.4951\n",
            "Precision@3: 0.5272\n",
            "\n",
            "SVM:\n",
            "Hamming Loss: 0.1648\n",
            "Micro-F1: 0.7176\n",
            "Macro-F1: 0.4748\n",
            "Precision@3: 0.4936\n",
            "\n",
            "Perceptron:\n",
            "Hamming Loss: 0.1538\n",
            "Micro-F1: 0.7401\n",
            "Macro-F1: 0.4971\n",
            "Precision@3: 0.5144\n",
            "\n",
            "DNN:\n",
            "Hamming Loss: 0.1120\n",
            "Micro-F1: 0.8029\n",
            "Macro-F1: 0.5323\n",
            "Precision@3: 0.5449\n",
            "\n",
            "Models saved successfully for Part 2!\n"
          ]
        }
      ]
    }
  ]
}